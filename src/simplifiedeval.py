# -*- coding: utf-8 -*-
"""SimplifiedEval.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_16Jw_MPmz3fw_TCK7f0mDv9CywEYOEG

## Installations
"""

from google.colab import drive
drive.mount('/content/drive')

"""This imports everything in the current google drive. Please note that the markdown cells which explain the code are generated by ChatGPT."""

!pip install datasets

"""This cell installs the `datasets` library, which is essential for accessing and processing large datasets used in the subsequent analysis.

## Probe-training

### GPU Playground
"""

import os
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset, random_split
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from datasets import Dataset
np.random.seed(42)

# Paths and constants
data_path = "/content/drive/MyDrive/cs4nlp-plmrb-main/data/processed"
out_path = "/content/drive/MyDrive/outputs_Hf_shuffle"
train_hash = "499193892"
test_hashes = ["336359147", "315634198"]
layers = ["initial", "middle", "final"]
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Load labels
train_labels_df = pd.read_csv("/content/drive/MyDrive/cs4nlp-plmrb-main/data/processed/train_dataset_processed.csv")
dataset = Dataset.from_pandas(train_labels_df)
dataset = dataset.shuffle(seed=42)

# Extract labels
labels = dataset['label']
# labels = train_labels_df["label"].values
# np.random.shuffle(labels)

# Convert labels to the format required by PyTorch
labels_torch = torch.tensor(labels, dtype=torch.float32).to(device)

# Logistic Regression Model in PyTorch
class LogisticRegressionModel(nn.Module):
    def __init__(self, input_dim):
        super(LogisticRegressionModel, self).__init__()
        self.linear = nn.Linear(input_dim, 1)

    def forward(self, x):
        return torch.sigmoid(self.linear(x))

# Deep Neural Network Model
class SimpleMLP(nn.Module):
    def __init__(self, input_dim):
        super(SimpleMLP, self).__init__()
        self.layers = nn.Sequential(
            nn.Linear(input_dim, 100),
            nn.ReLU(),
            nn.Linear(100, 100),
            nn.ReLU(),
            nn.Linear(100, 1),
            nn.Sigmoid()
        )

    def forward(self, x):
        return self.layers(x)


def train_model(model, train_loader, val_loader, epochs=10):
    criterion = nn.BCELoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)

    for epoch in range(epochs):
        model.train()
        for inputs, targets in train_loader:
            inputs, targets = inputs.to(device), targets.to(device).float()
            optimizer.zero_grad()
            outputs = model(inputs).squeeze()
            loss = criterion(outputs, targets)
            loss.backward()
            optimizer.step()

    model.eval()

    train_preds = []
    train_targets = []
    with torch.no_grad():
        for inputs, targets in train_loader:
            inputs = inputs.to(device)
            outputs = model(inputs).squeeze()
            train_preds.extend(outputs.cpu().numpy())
            train_targets.extend(targets.numpy())

    train_preds = np.round(train_preds)
    train_accuracy = accuracy_score(train_targets, train_preds)


    val_preds = []
    val_targets = []
    with torch.no_grad():
        for inputs, targets in val_loader:
            inputs = inputs.to(device)
            outputs = model(inputs).squeeze()
            val_preds.extend(outputs.cpu().numpy())
            val_targets.extend(targets.numpy())

    val_preds = np.round(val_preds)
    val_accuracy = accuracy_score(val_targets, val_preds)
    return train_accuracy, val_accuracy

def evaluate_model(model, test_loader):
    model.eval()
    test_preds = []
    with torch.no_grad():
        for inputs in test_loader:
            inputs = inputs.to(device)
            outputs = model(inputs).squeeze()
            test_preds.extend(outputs.cpu().numpy())

    test_preds = np.round(test_preds)
    return test_preds

def process_and_evaluate(layer, model_name, model, train_data, train_labels, val_data, val_labels, test_data_1, test_data_2, model_type="torch"):
    train_dataset = TensorDataset(torch.tensor(train_data, dtype=torch.float32), torch.tensor(train_labels, dtype=torch.float32))
    val_dataset = TensorDataset(torch.tensor(val_data, dtype=torch.float32), torch.tensor(val_labels, dtype=torch.float32))
    test_dataset_1 = TensorDataset(torch.tensor(test_data_1, dtype=torch.float32))
    test_dataset_2 = TensorDataset(torch.tensor(test_data_2, dtype=torch.float32))

    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)
    test_loader_1 = DataLoader(test_dataset_1, batch_size=32, shuffle=False)
    test_loader_2 = DataLoader(test_dataset_2, batch_size=32, shuffle=False)

    train_accuracy, val_accuracy = train_model(model, train_loader, val_loader)

    with torch.no_grad():
        test_preds_1_pos_probs = model(torch.tensor(test_data_1, dtype=torch.float32).to(device)).cpu().numpy().squeeze()
        test_preds_2_pos_probs = model(torch.tensor(test_data_2, dtype=torch.float32).to(device)).cpu().numpy().squeeze()

    test_preds_labels = pd.DataFrame({
        "pos_prob1": test_preds_1_pos_probs,
        "pos_prob2": test_preds_2_pos_probs
    })

    return train_accuracy, val_accuracy, test_preds_labels

for lm_folder in os.listdir(data_path):
    if (lm_folder=='GloveLanguageModel'):
        continue
    lm_path = os.path.join(data_path, lm_folder)
    out_lm_path = os.path.join(out_path, lm_folder)
    if os.path.isdir(lm_path):
        print("Processing data for", lm_folder)
        for layer in layers:
            print("Processing", layer, "layer")
            train_file = os.path.join(lm_path, f"mean_{layer}_{train_hash}.npy")
            test_file_1 = os.path.join(lm_path, f"mean_{layer}_{test_hashes[0]}.npy")
            test_file_2 = os.path.join(lm_path, f"mean_{layer}_{test_hashes[1]}.npy")

            train_data = np.load(train_file)
            test_data_1 = np.load(test_file_1)
            test_data_2 = np.load(test_file_2)

            train_data, val_data, train_labels, val_labels = train_test_split(train_data, labels, test_size=0.2, random_state=42)

            input_dim = train_data.shape[1]

            models = {
                # "logistic_regression": LogisticRegressionModel(input_dim).to(device),
                "mlp": SimpleMLP(input_dim).to(device),
            }

            model_types = {
                "logistic_regression": "torch",
                "mlp": "torch",
                "random_forest": "sklearn",
                "LSTM": "torch"
            }

            for model_name, model in models.items():
                output_dir = os.path.join(out_lm_path, model_name, layer)
                if os.path.isdir(output_dir):
                  continue
                print("Training", model_name, "model")
                train_accuracy, val_accuracy, test_preds_probs = process_and_evaluate(layer, model_name, model, train_data, train_labels, val_data, val_labels, test_data_1, test_data_2, model_types[model_name])
                print("Training for", model_name, "model complete!")

                os.makedirs(output_dir, exist_ok=True)

                test_preds_probs.to_csv(os.path.join(output_dir, "test_preds_probs.csv"), index=False)

                with open(os.path.join(output_dir, "accuracy.txt"), 'w') as f:
                    f.write(f"Training Accuracy: {train_accuracy}\nValidation Accuracy: {val_accuracy}\n")

"""This script begins by importing necessary libraries for data manipulation, machine learning, and neural network operations, including `numpy`, `pandas`, `torch`, and `sklearn`, along with setting a random seed for reproducibility. It defines paths and constants for data and output directories, dataset identifiers, and layer names, and determines whether to use a GPU or CPU. The training labels are loaded from a CSV file into a pandas DataFrame, converted into a Hugging Face `Dataset`, and shuffled. The labels are extracted and converted to a PyTorch tensor for use in training. Two PyTorch neural network models are defined: a logistic regression model, and a simple multi-layer perceptron (MLP), each with specified architectures and forward pass methods. The `train_model` function trains a given model using training data and evaluates it on validation data, returning accuracy scores. The `evaluate_model` function generates predictions on test data. The `process_and_evaluate` function processes the data, trains the model, and evaluates it on training, validation, and two test datasets (with and without mask), returning accuracies and a DataFrame of predicted probabilities. The main loop iterates over directories in the data path, processes data for each specified layer, loads the training and test data, splits the training data into training and validation sets, and initializes a dictionary of models to be trained. For each model, the `process_and_evaluate` function is called to train and evaluate the model, with results saved to the specified output directory.

### CPU Playground
"""

import os
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
from datasets import Dataset

# Paths and constants
data_path = "/content/drive/MyDrive/cs4nlp-plmrb-main/data/processed"
out_path = "/content/drive/MyDrive/outputs_Hf_shuffle"
train_hash = "499193892"
test_hashes = ["336359147", "315634198"]
layers = ["initial", "middle", "final"]

# Load labels
train_labels_df = pd.read_csv("/content/drive/MyDrive/cs4nlp-plmrb-main/data/processed/train_dataset_processed.csv")
dataset = Dataset.from_pandas(train_labels_df)
dataset = dataset.shuffle(seed=42)

# Extract labels
labels = dataset['label']

def process_and_evaluate(layer, model_name, model, train_data, train_labels, val_data, val_labels, test_data_1, test_data_2):
    model.fit(train_data, train_labels)
    val_preds = model.predict(val_data)
    val_accuracy = accuracy_score(val_labels, val_preds)
    train_preds = model.predict(train_data)
    train_accuracy = accuracy_score(train_labels, train_preds)

    test_data_1_pred_labels = model.predict(test_data_1)
    test_data_1_pred_pos_probs = model.predict_proba(test_data_1)[:, 1]

    test_data_2_pred_labels = model.predict(test_data_2)
    test_data_2_pred_pos_probs = model.predict_proba(test_data_2)[:, 1]

    test_preds_labels = pd.DataFrame({
        "pos_prob1": test_data_1_pred_pos_probs,
        "pos_prob2": test_data_2_pred_pos_probs
    })
    # During visualization or wtv, map negative to -1 and positive to +1

    return train_accuracy, val_accuracy, test_preds_labels

for lm_folder in os.listdir(data_path):
    if lm_folder=='GloveLanguageModel':
        continue
    lm_path = os.path.join(data_path, lm_folder)
    out_lm_path = os.path.join(out_path, lm_folder)
    if os.path.isdir(lm_path):
        print("Processing data for", lm_folder)
        for layer in layers:
            print("\tProcessing", layer, "layer")
            train_file = os.path.join(lm_path, f"mean_{layer}_{train_hash}.npy")
            # Train data has 0-1 labels
            test_data_file_1 = os.path.join(lm_path, f"mean_{layer}_{test_hashes[0]}.npy")
            test_data_file_2 = os.path.join(lm_path, f"mean_{layer}_{test_hashes[1]}.npy")
            # Test data has -1, 1 labels

            train_data = np.load(train_file)
            test_data_1 = np.load(test_data_file_1)
            test_data_2 = np.load(test_data_file_2)

            train_data, val_data, train_labels, val_labels = train_test_split(train_data, labels, test_size=0.2, random_state=42)

            models = {
                # "logistic_regression": LogisticRegression(max_iter=1000),
                # "svm": SVC(probability=True),
                # "random_forest": RandomForestClassifier(n_estimators=50, random_state=42),
                # "k-nn": KNeighborsClassifier(n_neighbors=100),
                "gaussian_nb": GaussianNB()
                # "mlp": MLPClassifier(hidden_layer_sizes=(100, 100), max_iter=1000)
            }
            for model_name, model in models.items():
                print("\t\tTraining", model_name, "model")
                train_accuracy, val_accuracy, test_preds_probs = process_and_evaluate(layer, model_name, model, train_data, train_labels, val_data, val_labels, test_data_1, test_data_2)
                print("\t\tTraining for", model_name, "model complete!")

                output_dir = os.path.join(out_lm_path, model_name, layer)
                os.makedirs(output_dir, exist_ok=True)

                test_preds_probs.to_csv(os.path.join(output_dir, "test_preds_probs.csv"), index=False)

                with open(os.path.join(output_dir, "accuracy.txt"), 'w') as f:
                    f.write(f"Training Accuracy: {train_accuracy}\n")
                    f.write(f"Validation Accuracy: {val_accuracy}\n")

"""This script begins by importing necessary libraries for data manipulation and machine learning, such as `numpy`, `pandas`, `sklearn`, and `datasets`, and sets up paths and constants for data and output directories, dataset identifiers, and layer names. Training labels are loaded from a CSV file into a pandas DataFrame, converted into a Hugging Face `Dataset`, and shuffled. Labels are extracted from the dataset. The `process_and_evaluate` function is defined to train a given model using training data, evaluate it on validation data, and generate predictions on two test datasets, returning accuracy scores and a DataFrame of predicted probabilities. The main loop iterates over directories in the data path, skipping the `GloveLanguageModel` folder. For each language model folder, it processes data for each specified layer, loads training and test data from `.npy` files, and splits the training data into training and validation sets. A dictionary of models (`models`) is created, including Gaussian Naive Bayes. For each model, the `process_and_evaluate` function is called to train and evaluate the model. Results, including test predictions and accuracies, are saved to the specified output directory.

### Glove CPU playground
"""

import os
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
from datasets import Dataset
import shutil

# Paths and constants
data_path = "/content/drive/MyDrive/cs4nlp-plmrb-main/data/processed"
out_path = "/content/drive/MyDrive/outputs_Hf_shuffle"
train_hash = "499193892"
test_hashes = ["336359147", "315634198"]
layers = ["initial", "middle", "final"]

# Load labels
train_labels_df = pd.read_csv("/content/drive/MyDrive/cs4nlp-plmrb-main/data/processed/train_dataset_processed.csv")
dataset = Dataset.from_pandas(train_labels_df)
dataset = dataset.shuffle(seed=42)

# Extract labels
labels = dataset['label']

def process_and_evaluate(layer, model_name, model, train_data, train_labels, val_data, val_labels, test_data_1, test_data_2):
    model.fit(train_data, train_labels)
    val_preds = model.predict(val_data)
    val_accuracy = accuracy_score(val_labels, val_preds)
    train_preds = model.predict(train_data)
    train_accuracy = accuracy_score(train_labels, train_preds)

    test_data_1_pred_labels = model.predict(test_data_1)
    test_data_1_pred_pos_probs = model.predict_proba(test_data_1)[:, 1]

    test_data_2_pred_labels = model.predict(test_data_2)
    test_data_2_pred_pos_probs = model.predict_proba(test_data_2)[:, 1]

    test_preds_labels = pd.DataFrame({
        "pos_prob1": test_data_1_pred_pos_probs,
        "pos_prob2": test_data_2_pred_pos_probs
    })
    # During visualization or wtv, map negative to -1 and positive to +1

    return train_accuracy, val_accuracy, test_preds_labels

for lm_folder in os.listdir(data_path):
    if lm_folder!='GloveLanguageModel':
        continue
    lm_path = os.path.join(data_path, lm_folder)
    out_lm_path = os.path.join(out_path, lm_folder)
    if os.path.isdir(lm_path):
        print("Processing data for", lm_folder)
        for layer in layers:
            if layer=='initial':
              print("\tProcessing", layer, "layer")
              train_file = os.path.join(lm_path, f"{layer}_{train_hash}.npy")
              # Train data has 0-1 labels
              test_data_file_1 = os.path.join(lm_path, f"{layer}_{test_hashes[0]}.npy")
              test_data_file_2 = os.path.join(lm_path, f"{layer}_{test_hashes[1]}.npy")
              # Test data has -1, 1 labels

              train_data = np.load(train_file)
              test_data_1 = np.load(test_data_file_1)
              test_data_2 = np.load(test_data_file_2)

              train_data, val_data, train_labels, val_labels = train_test_split(train_data, labels, test_size=0.2, random_state=42)

              models = {
                  # "logistic_regression": LogisticRegression(max_iter=1000),
                  # "svm": SVC(probability=True),
                  "random_forest": RandomForestClassifier(n_estimators=50, random_state=42),
                  # "k-nn": KNeighborsClassifier(n_neighbors=100),
                  "gaussian_nb": GaussianNB()
                  # "mlp": MLPClassifier(hidden_layer_sizes=(100, 100), max_iter=1000)
              }
              for model_name, model in models.items():
                  print("\t\tTraining", model_name, "model")
                  train_accuracy, val_accuracy, test_preds_probs = process_and_evaluate(layer, model_name, model, train_data, train_labels, val_data, val_labels, test_data_1, test_data_2)
                  print("\t\tTraining for", model_name, "model complete!")

                  output_dir = os.path.join(out_lm_path, model_name, layer)
                  os.makedirs(output_dir, exist_ok=True)

                  test_preds_probs.to_csv(os.path.join(output_dir, "test_preds_probs.csv"), index=False)

                  with open(os.path.join(output_dir, "accuracy.txt"), 'w') as f:
                      f.write(f"Training Accuracy: {train_accuracy}\n")
                      f.write(f"Validation Accuracy: {val_accuracy}\n")

                  layer_alt = ['middle', 'final']
                  for layer_mf in layer_alt:
                      alt_output_dir = os.path.join(out_lm_path, model_name, layer_mf)
                      os.makedirs(alt_output_dir, exist_ok=True)

                      test_preds_probs.to_csv(os.path.join(alt_output_dir, "test_preds_probs.csv"), index=False)

                      with open(os.path.join(alt_output_dir, "accuracy.txt"), 'w') as f:
                          f.write(f"Training Accuracy: {train_accuracy}\n")
                          f.write(f"Validation Accuracy: {val_accuracy}\n")

"""The script starts by importing necessary libraries for data manipulation and machine learning, including `numpy`, `pandas`, `sklearn`, and `datasets`. Paths and constants are set up for data and output directories, dataset identifiers, and layer names. Training labels are loaded from a CSV file into a pandas DataFrame, converted into a Hugging Face `Dataset`, and shuffled. Labels are extracted from the dataset. The `process_and_evaluate` function is defined to train a given model using training data, evaluate it on validation data, and generate predictions on two test datasets, returning accuracy scores and a DataFrame of predicted probabilities. The main loop iterates over directories in the data path, focusing only on the `GloveLanguageModel` folder. For each specified layer, the script loads training and test data from `.npy` files, and splits the training data into training and validation sets. A dictionary of models is created, including Random Forest and Gaussian Naive Bayes. For each model, the `process_and_evaluate` function is called to train and evaluate the model. Results, including test predictions and accuracies, are saved to the specified output directory. Additionally, results for the "middle" and "final" layers are saved using the same trained model, ensuring consistency across layers.

### Glove GPU playground
"""

import os
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset, random_split
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from datasets import Dataset
np.random.seed(42)

# Paths and constants
data_path = "/content/drive/MyDrive/cs4nlp-plmrb-main/data/processed"
out_path = "/content/drive/MyDrive/outputs_Hf_shuffle"
train_hash = "499193892"
test_hashes = ["336359147", "315634198"]
layers = ["initial", "middle", "final"]
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Load labels
train_labels_df = pd.read_csv("/content/drive/MyDrive/cs4nlp-plmrb-main/data/processed/train_dataset_processed.csv")
dataset = Dataset.from_pandas(train_labels_df)
dataset = dataset.shuffle(seed=42)

# Extract labels
labels = dataset['label']
# labels = train_labels_df["label"].values
# np.random.shuffle(labels)

# Convert labels to the format required by PyTorch
labels_torch = torch.tensor(labels, dtype=torch.float32).to(device)

# Logistic Regression Model in PyTorch
class LogisticRegressionModel(nn.Module):
    def __init__(self, input_dim):
        super(LogisticRegressionModel, self).__init__()
        self.linear = nn.Linear(input_dim, 1)

    def forward(self, x):
        return torch.sigmoid(self.linear(x))

# Deep Neural Network Model
class SimpleMLP(nn.Module):
    def __init__(self, input_dim):
        super(SimpleMLP, self).__init__()
        self.layers = nn.Sequential(
            nn.Linear(input_dim, 100),
            nn.ReLU(),
            nn.Linear(100, 100),
            nn.ReLU(),
            nn.Linear(100, 1),
            nn.Sigmoid()
        )

    def forward(self, x):
        return self.layers(x)

def train_model(model, train_loader, val_loader, epochs=10):
    criterion = nn.BCELoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)

    for epoch in range(epochs):
        model.train()
        for inputs, targets in train_loader:
            inputs, targets = inputs.to(device), targets.to(device).float()
            optimizer.zero_grad()
            outputs = model(inputs).squeeze()
            loss = criterion(outputs, targets)
            loss.backward()
            optimizer.step()

    model.eval()

    train_preds = []
    train_targets = []
    with torch.no_grad():
        for inputs, targets in train_loader:
            inputs = inputs.to(device)
            outputs = model(inputs).squeeze()
            train_preds.extend(outputs.cpu().numpy())
            train_targets.extend(targets.numpy())

    train_preds = np.round(train_preds)
    train_accuracy = accuracy_score(train_targets, train_preds)


    val_preds = []
    val_targets = []
    with torch.no_grad():
        for inputs, targets in val_loader:
            inputs = inputs.to(device)
            outputs = model(inputs).squeeze()
            val_preds.extend(outputs.cpu().numpy())
            val_targets.extend(targets.numpy())

    val_preds = np.round(val_preds)
    val_accuracy = accuracy_score(val_targets, val_preds)
    return train_accuracy, val_accuracy

def evaluate_model(model, test_loader):
    model.eval()
    test_preds = []
    with torch.no_grad():
        for inputs in test_loader:
            inputs = inputs.to(device)
            outputs = model(inputs).squeeze()
            test_preds.extend(outputs.cpu().numpy())

    test_preds = np.round(test_preds)
    return test_preds

def process_and_evaluate(layer, model_name, model, train_data, train_labels, val_data, val_labels, test_data_1, test_data_2, model_type="torch"):
    train_dataset = TensorDataset(torch.tensor(train_data, dtype=torch.float32), torch.tensor(train_labels, dtype=torch.float32))
    val_dataset = TensorDataset(torch.tensor(val_data, dtype=torch.float32), torch.tensor(val_labels, dtype=torch.float32))
    test_dataset_1 = TensorDataset(torch.tensor(test_data_1, dtype=torch.float32))
    test_dataset_2 = TensorDataset(torch.tensor(test_data_2, dtype=torch.float32))

    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)
    test_loader_1 = DataLoader(test_dataset_1, batch_size=32, shuffle=False)
    test_loader_2 = DataLoader(test_dataset_2, batch_size=32, shuffle=False)

    train_accuracy, val_accuracy = train_model(model, train_loader, val_loader)
    # test_preds_1 = evaluate_model(model, test_loader_1)
    # test_preds_2 = evaluate_model(model, test_loader_2)

    with torch.no_grad():
        test_preds_1_pos_probs = model(torch.tensor(test_data_1, dtype=torch.float32).to(device)).cpu().numpy().squeeze()
        test_preds_2_pos_probs = model(torch.tensor(test_data_2, dtype=torch.float32).to(device)).cpu().numpy().squeeze()

    test_preds_labels = pd.DataFrame({
        "pos_prob1": test_preds_1_pos_probs,
        "pos_prob2": test_preds_2_pos_probs
    })

    return train_accuracy, val_accuracy, test_preds_labels

for lm_folder in os.listdir(data_path):
    if (lm_folder!='GloveLanguageModel'):
        continue
    lm_path = os.path.join(data_path, lm_folder)
    out_lm_path = os.path.join(out_path, lm_folder)
    if os.path.isdir(lm_path):
        print("Processing data for", lm_folder)
        for layer in layers:
            if layer=='initial':
                print("\tProcessing", layer, "layer")
                train_file = os.path.join(lm_path, f"{layer}_{train_hash}.npy")
                test_file_1 = os.path.join(lm_path, f"{layer}_{test_hashes[0]}.npy")
                test_file_2 = os.path.join(lm_path, f"{layer}_{test_hashes[1]}.npy")

                train_data = np.load(train_file)
                test_data_1 = np.load(test_file_1)
                test_data_2 = np.load(test_file_2)

                train_data, val_data, train_labels, val_labels = train_test_split(train_data, labels, test_size=0.2, random_state=42)

                input_dim = train_data.shape[1]

                models = {
                    "logistic_regression": LogisticRegressionModel(input_dim).to(device),
                    "mlp": SimpleMLP(input_dim).to(device),
                    # "random_forest": RandomForestClassifier(n_estimators=100, random_state=42)
                }

                model_types = {
                    "logistic_regression": "torch",
                    "mlp": "torch",
                    "random_forest": "sklearn"
                }

                for model_name, model in models.items():
                    output_dir = os.path.join(out_lm_path, model_name, layer)
                    print("\t\tTraining", model_name, "model")
                    train_accuracy, val_accuracy, test_preds_probs = process_and_evaluate(layer, model_name, model, train_data, train_labels, val_data, val_labels, test_data_1, test_data_2, model_types[model_name])
                    print("\t\tTraining for", model_name, "model complete!")

                    os.makedirs(output_dir, exist_ok=True)

                    test_preds_probs.to_csv(os.path.join(output_dir, "test_preds_probs.csv"), index=False)

                    with open(os.path.join(output_dir, "accuracy.txt"), 'w') as f:
                        f.write(f"Training Accuracy: {train_accuracy}\nValidation Accuracy: {val_accuracy}\n")

                    layer_alt = ['middle', 'final']

                    for layer_mf in layer_alt:
                        alt_output_dir = os.path.join(out_lm_path, model_name, layer_mf)
                        os.makedirs(alt_output_dir, exist_ok=True)

                        test_preds_probs.to_csv(os.path.join(alt_output_dir, "test_preds_probs.csv"), index=False)

                        with open(os.path.join(alt_output_dir, "accuracy.txt"), 'w') as f:
                            f.write(f"Training Accuracy: {train_accuracy}\n")
                            f.write(f"Validation Accuracy: {val_accuracy}\n")

"""The script imports essential libraries for data manipulation, machine learning, and deep learning, including `numpy`, `pandas`, `torch`, and `sklearn`. Paths and constants are set for data and output directories, dataset identifiers, and layer names. The script then checks for GPU availability and assigns the device accordingly. Training labels are loaded from a CSV file into a pandas DataFrame, converted into a Hugging Face `Dataset`, shuffled, and labels are extracted. These labels are converted into a PyTorch tensor for further processing.

Two PyTorch models are defined: a `LogisticRegressionModel` and a `SimpleMLP` (a simple multi-layer perceptron). A function `train_model` is defined to handle the training process of the models, which includes setting up the loss function and optimizer, iterating through epochs, and calculating training and validation accuracies. The `evaluate_model` function is defined to handle the evaluation of models on test data, but it’s not used directly within the `process_and_evaluate` function.

The `process_and_evaluate` function prepares the data by creating PyTorch `TensorDataset` and `DataLoader` objects for training, validation, and test datasets. It then trains the model using the `train_model` function and generates predictions on the test datasets, returning the training accuracy, validation accuracy, and a DataFrame of predicted probabilities for the test datasets.

The main loop iterates over directories in the data path, focusing only on the `GloveLanguageModel` folder. For each specified layer, it processes the "initial" layer by loading training and test data from `.npy` files, splitting the training data into training and validation sets, and defining the input dimension for the models. A dictionary of models is created, including `LogisticRegressionModel` and `SimpleMLP`.

For each model, the `process_and_evaluate` function is called to train and evaluate the model. Results, including test predictions and accuracies, are saved to the specified output directory. Additionally, results for the "middle" and "final" layers are saved using the same trained model, ensuring consistency across layers. This ensures that the models' performance is recorded and evaluated across different data representations.

## test_preds to CSV
"""

"""1 is NEUTRAL, 2 is regular!!"""

import os
import pandas as pd


def process_csv(test_preds_probs_path, output_test_preds_probs_path, constant_csv_path):
    # Read the test_preds_probs.csv file
    test_preds_df = pd.read_csv(test_preds_probs_path)

    # Extract the columns pos_probs1 and pos_probs2
    try:
      pos_probs1 = test_preds_df['pos_prob1']
      pos_probs2 = test_preds_df['pos_prob2']
    except:
      pos_probs1 = test_preds_df['prob1']
      pos_probs2 = test_preds_df['prob2']

    # Read the constant CSV file
    constant_df = pd.read_csv(constant_csv_path)
    constant_df.rename(columns={'input_neutral':'input_with_mask'}, inplace=True)

    # Add the new columns to the constant DataFrame
    constant_df['pred_label_pos_prob'] = pos_probs2
    constant_df['pred_label_pos_prob_with_mask'] = pos_probs1

    # Compute the pred_label and pred_label_with_mask columns
    constant_df['pred_label'] = constant_df['pred_label_pos_prob'].apply(lambda x: 1 if x >= 0.5 else -1)
    constant_df['pred_label_with_mask'] = constant_df['pred_label_pos_prob_with_mask'].apply(lambda x: 1 if x >= 0.5 else -1)

    # Save the modified DataFrame back to the original location
    constant_df.to_csv(output_test_preds_probs_path, index=False)

def traverse_and_process(data_dir, constant_csv_path):
    for lm_folder in os.listdir(data_dir):
        # if lm_folder=='GloveLanguageModel':
        #     continue
        print("Inside", lm_folder)
        lm_path = os.path.join(data_dir, lm_folder)
        for probe_folder in os.listdir(lm_path):
            print("\tInside", probe_folder)
            probe_path = os.path.join(lm_path, probe_folder)
            for layer_folder in os.listdir(probe_path):
                print("\t\tInside", layer_folder)
                layer_path = os.path.join(probe_path, layer_folder)
                for root, dirs, files in os.walk(layer_path):
                    for file in files:
                        if file == 'test_preds_probs.csv':
                            test_preds_probs_path = os.path.join(root, file)
                            output_test_preds_probs_path = os.path.join(layer_path, 'test_data_evaluated.csv')
                            process_csv(test_preds_probs_path, output_test_preds_probs_path, constant_csv_path)

# Define the constant CSV file path for test data
constant_csv_path = '/content/drive/MyDrive/cs4nlp-plmrb-main/data/processed/generated_eval_dataset_with_mask.csv'
# Define the root data directory
data_dir = '/content/drive/MyDrive/outputs_Hf_shuffle'

# Traverse the directory structure and process each test_preds_probs.csv file
traverse_and_process(data_dir, constant_csv_path)

"""The script is designed to process CSV files within a specified directory structure by identifying and manipulating `test_preds_probs.csv` files. Initially, it reads these files to extract probability columns `pos_prob1` and `pos_prob2`, or alternatively `prob1` and `prob2`, if the former are not found. These probabilities are then integrated into a constant CSV file (`generated_eval_dataset_with_mask.csv`) after renaming one of its columns from `input_neutral` to `input_with_mask`. The script adds new columns `pred_label_pos_prob` and `pred_label_pos_prob_with_mask` to the constant DataFrame, which store the extracted probabilities. Additionally, it computes the `pred_label` and `pred_label_with_mask` columns by applying a threshold of 0.5 to the probabilities, assigning a value of 1 if the probability is equal to or greater than 0.5, and -1 otherwise. The modified DataFrame, which now contains the augmented data, is saved back to the directory as `test_data_evaluated.csv`. The script features a `process_csv` function that handles the reading, processing, and writing of CSV files, and a `traverse_and_process` function that recursively navigates the directory structure, applying `process_csv` to each relevant `test_preds_probs.csv` file it encounters. This approach ensures that all CSV files within the given directory are systematically processed and updated according to the defined logic.

## CSVs to Dictionary
"""

import os
import pandas as pd
import json

# pred_label_pos_prob, pred_label_pos_prob_with_mask also exist

def demographic_parity(df, gender_column='group', prediction_column='pred_label'):
    mean_prediction = df.groupby(gender_column)[prediction_column].mean()
    return mean_prediction

def equalized_odds(df, gender_column='group', true_column='TrueSentiment', prediction_column='pred_label'):
    ez_o = df.groupby(gender_column)[prediction_column].mean()
    return ez_o

def relative_sentiment_change(df, mask_prediction_column='pred_label_with_mask', gender_column='group',
                              true_column='TrueSentiment', prediction_column='pred_label'):
    changes = []
    for _, row in df.iterrows():
        original_sentiment = row[prediction_column]
        mask_sentiment = row[mask_prediction_column]
        change = mask_sentiment - original_sentiment
        changes.append((row[gender_column], change))
    return pd.DataFrame(changes, columns=[gender_column, 'Relative Sentiment Change'])

def evaluate(evalDataPath):
    """Calculates metrics based on the real and predicted sentiments"""

    # FOR LABELS
    evalData = pd.read_csv(evalDataPath)
    evalData['pred_label_neg_prob'] = 1 - evalData['pred_label_pos_prob']
    data_negative = evalData[evalData['label'] == -1]
    data_positive = evalData[evalData['label'] == 1]
    data_binary = evalData[evalData['label'] != 0]
    data_neutral = evalData[evalData['label'] == 0]

    # Accuracy
    correct_predictions = (data_binary['label'] == data_binary['pred_label']).sum()
    acc = correct_predictions/len(data_binary)

    # Demographic Parity
    dp = demographic_parity(data_binary, gender_column='group', prediction_column='pred_label')

    # Equalized Odds
    ez_o_neg = equalized_odds(data_negative, gender_column='group', true_column='label', prediction_column='pred_label')
    ez_o_pos = equalized_odds(data_positive, gender_column='group', true_column='label', prediction_column='pred_label')

    # Relative Sentiment Change for neutral examples
    relative_changes = relative_sentiment_change(data_neutral, mask_prediction_column='pred_label_with_mask', gender_column='group',
                              true_column='label', prediction_column='pred_label')
    average_changes = relative_changes.groupby('group')['Relative Sentiment Change'].mean()





    # FOR PROBS
    evalData = pd.read_csv(evalDataPath)
    data_negative = evalData[evalData['label'] == -1]
    data_positive = evalData[evalData['label'] == 1]
    data_binary = evalData[evalData['label'] != 0]
    data_neutral = evalData[evalData['label'] == 0]

    # Demographic Parity
    dp_prob = demographic_parity(data_binary, gender_column='group', prediction_column='pred_label_pos_prob')

    # Equalized Odds
    ez_o_neg_prob = equalized_odds(data_negative, gender_column='group', true_column='label', prediction_column='pred_label_pos_prob')
    ez_o_pos_prob = equalized_odds(data_positive, gender_column='group', true_column='label', prediction_column='pred_label_pos_prob')

    # Relative Sentiment Change for neutral examples
    relative_changes_prob = relative_sentiment_change(data_neutral, mask_prediction_column='pred_label_pos_prob_with_mask', gender_column='group',
                              true_column='label', prediction_column='pred_label_pos_prob')
    average_changes_prob = relative_changes_prob.groupby('group')['Relative Sentiment Change'].mean()



    return acc, dp, ez_o_neg, ez_o_pos, average_changes, dp_prob, ez_o_neg_prob, ez_o_pos_prob, average_changes_prob


def read_and_format_accuracies(file_path):
    with open(file_path, 'r') as file:
        lines = file.readlines()

    accuracies = {}
    for line in lines:
        metric, value = line.strip().split(':')
        accuracies[metric.strip()] = f"{float(value) * 100:.2f}"

    return accuracies


def traverse_and_evaluate(data_dir):
    results = {}
    for lm_folder in os.listdir(data_dir):
        # if lm_folder=='GloveLanguageModel':
        #     continue
        print("Inside", lm_folder)
        lm_path = os.path.join(data_dir, lm_folder)
        for probe_folder in os.listdir(lm_path):
            print("\tInside", probe_folder)
            probe_path = os.path.join(lm_path, probe_folder)
            for layer_folder in os.listdir(probe_path):
                print("\t\tInside", layer_folder)
                layer_path = os.path.join(probe_path, layer_folder)
                for root, dirs, files in os.walk(layer_path):
                    for file in files:
                        if file == 'test_data_evaluated.csv':
                            accuracy_file = os.path.join(root, 'accuracy.txt')
                            accuracies = read_and_format_accuracies(accuracy_file)

                            lm = lm_folder
                            ml_model = probe_folder
                            layer = layer_folder

                            test_preds_probs_path = os.path.join(root, file)
                            acc, dp, ez_o_neg, ez_o_pos, avg_changes, dp_prob, ez_o_neg_prob, ez_o_pos_prob, avg_changes_prob = evaluate(test_preds_probs_path)

                            if lm not in results:
                                results[lm] = {}
                            if ml_model not in results[lm]:
                                results[lm][ml_model] = {}
                            if layer not in results[lm][ml_model]:
                                results[lm][ml_model][layer] = {}

                            results[lm][ml_model][layer] = {
                                'Training Accuracy': accuracies['Training Accuracy'],
                                'Validation Accuracy': accuracies['Validation Accuracy'],
                                'Test Accuracy': acc,
                                'Demographic Parity (Label)': dp.to_dict(),
                                'Equalized Odds Negative (Label)': ez_o_neg.to_dict(),
                                'Equalized Odds Positive (Label)': ez_o_pos.to_dict(),
                                'Average Relative Sentiment Change (Label)': avg_changes.to_dict(),
                                'Demographic Parity (Prob)': dp_prob.to_dict(),
                                'Equalized Odds Negative (Prob)': ez_o_neg_prob.to_dict(),
                                'Equalized Odds Positive (Prob)': ez_o_pos_prob.to_dict(),
                                'Average Relative Sentiment Change (Prob)': avg_changes_prob.to_dict(),
                            }

    return results

# Define the root data directory
data_dir = '/content/drive/MyDrive/outputs_Hf_shuffle'

# Traverse the directory structure and evaluate each test_preds_probs.csv file
results = traverse_and_evaluate(data_dir)
with open("IDidAThing.json","w") as f:
    json.dump(results,f)
# Printing the results for verification
import pprint
pprint.pprint(results)

"""This script is designed to evaluate machine learning models by calculating various metrics from CSV files containing prediction results. It begins by importing necessary libraries such as `os`, `pandas`, and `json`. The script defines functions for calculating key metrics: `demographic_parity`, `equalized_odds`, and `relative_sentiment_change`, which compute the mean predictions, equalized odds, and sentiment changes, respectively, grouped by gender.

The `evaluate` function reads an evaluation data CSV file, processes it to calculate the defined metrics for both labels and probabilities, and returns these metrics. It first calculates the negative probabilities and separates the data into negative, positive, binary, and neutral subsets. Accuracy is calculated by comparing true and predicted labels. The function also calculates demographic parity and equalized odds for negative and positive sentiments, and the average relative sentiment change for neutral examples.

The `read_and_format_accuracies` function reads accuracy values from a text file and formats them as percentages. The `traverse_and_evaluate` function navigates through a directory structure to locate `test_data_evaluated.csv` files, read the corresponding `accuracy.txt` files, and apply the `evaluate` function to compute the metrics. It organizes the results in a nested dictionary structure based on language model, machine learning model, and layer.

Finally, the script sets the root data directory and invokes the `traverse_and_evaluate` function to process the evaluation files, saving the results as a JSON file for further analysis and printing them for verification. This comprehensive approach ensures thorough evaluation and systematic organization of results across different models and layers.
"""